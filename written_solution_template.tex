\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}

\usepackage[shortlabels]{enumitem}
\usepackage[noabbrev, capitalise]{cleveref}

\usepackage{geometry}
 \geometry{
 a4paper,
 top=20mm,
 bottom=25mm,
 left=25mm,
 right=25mm,
 }

% define your IDs here:
\newcommand{\firststudentid}{123456789}
\newcommand{\secondstudentid}{987654321}

\pagestyle{fancy}
\fancyhf{}
\rhead{Written Solution for Assignment 1}
\chead{\firststudentid \qquad \secondstudentid}
\lhead{Natural Language Processing}
\rfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}
\renewcommand{\footrulewidth}{1pt}
 
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1.25}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\roman{subsubsection}}

\begin{document}


\section{Word-Level Neural Bigram Language Model}


\begin{itemize}
    \item[(a)]
    
    \begin{align*}
        \text{CE}(\mathbf{y}, \hat{\mathbf{y}}) &= \text{CE}(\mathbf{y}, \text{softmax}(\boldsymbol{\theta})) \\
        &= \sum_i y_i \ln (\text{softmax}(\boldsymbol{\theta})_i) \\
        &= \sum_i y_i \ln \left( \frac{\exp(\theta_i)}{\sum_j \exp(\theta_j)} \right) \\
        &= \sum_i y_i \left(\theta_i - \ln\left(\sum_j \exp(\theta_j)\right)\right) \\
        &= \mathbf{y} \cdot \boldsymbol{\theta} - \sum_i y_i \ln(\sum_j \exp(\theta_j))) \\
    \end{align*}
    
    Since $\mathbf{y}$ is one-hot encoded, $\sum_i y_i = 1$, so
    
    \begin{align*}
        \text{CE}(\mathbf{y}, \hat{\mathbf{y}}) &= \mathbf{y} \cdot \boldsymbol{\theta} - \ln(\sum_j \exp(\theta_j)) \\
        \frac{\partial}{\partial \theta_i} \text{CE}(\mathbf{y}, \hat{\mathbf{y}}) &= y_i - \frac{\exp(\theta_i)}{\sum_j \exp(\theta_j)} \\
        &= y_i - \text{softmax}(\boldsymbol{\theta})_i \\
        \nabla_{\boldsymbol{\theta}} \text{CE}(\mathbf{y}, \hat{\mathbf{y}}) &= \mathbf{y} - \text{softmax}(\boldsymbol{\theta})
    \end{align*}
    
    This makes sense since the unique critical point (point where gradient equals zero) occurs where $\text{softmax}(\boldsymbol{\theta}) = \mathbf{y}$.
    
    \item[(b)]
    
    Define $\boldsymbol{\theta}_1 = \mathbf{x}\mathbf{W}_1 + \mathbf{b}_1$ and $\boldsymbol{\theta}_2 = \mathbf{h}\mathbf{W}_2 + \mathbf{b}_2$ (logit vectors), and note that these have Jacobians $\frac{\partial \boldsymbol{\theta}_1}{\partial \mathbf{x}} = \mathbf{W}_1^T$ and $\frac{\partial \boldsymbol{\theta}_2}{\partial \mathbf{h}} = \mathbf{W}_2^T$.
    
    For any vector $\mathbf{v}$, define $\boldsymbol{\Sigma}(\mathbf{v})$ to be the diagonal matrix with $i$-th diagonal element equal to $\sigma(v_i)(1-\sigma(v_i))$. By homework 1 question 2e, $\boldsymbol{\Sigma}(\mathbf{v})$ is the Jacobian matrix $\frac{\boldsymbol{\Sigma}(\mathbf{v})}{\partial \mathbf{v}}$.
    
    By the result in (a) and the chain rule,
    
        $$\frac{\partial J(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{h}} = \frac{\partial J(\mathbf{y}, \hat{\mathbf{y}})}{\partial \boldsymbol{\theta_2}} \frac{\partial \boldsymbol{\theta_2}}{\partial \mathbf{h}} \\
        = (\mathbf{y} - \text{softmax}(\boldsymbol{\theta_2})) \mathbf{W}_2^T$$
    
    Therefore, using that $\mathbf{h} = \boldsymbol{\Sigma}(\mathbf{\theta_1})$ and applying the chain rule repeatedly,
    
    \begin{align*}
    \frac{\partial J(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{x}} &= \frac{\partial J(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{h}} \frac{\partial \mathbf{h}}{\partial \mathbf{x}} \\
    &= \frac{\partial J(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{h}} \frac{\partial \boldsymbol{\Sigma}(\boldsymbol{\theta}_1)}{\partial \mathbf{x}} \\
    &= \frac{\partial J(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{h}} \frac{\partial \boldsymbol{\Sigma}(\boldsymbol{\theta}_1)}{\partial \boldsymbol{\theta}_1} \frac{\partial \boldsymbol{\theta}_1}{\mathbf{x}} \\
    &= (\mathbf{y} - \text{softmax}(\boldsymbol{\theta_2})) \mathbf{W}_2^T \boldsymbol{\Sigma}(\boldsymbol{\theta}_1) \mathbf{W}_1^T \\
    \end{align*}
    
    Note that in this expression the matrix shapes are compatible: $\mathbf{y}-\text{softmax}(\boldsymbol{\theta}_2)$ is $1 \times D_y$, $\mathbf{W}_2^T$ is $D_y \times D_h$, $\boldsymbol{\Sigma}(\boldsymbol{\theta}_1)$ is $D_h \times D_h$, and $\mathbf{W}_1^T$ is $D_h \times D_x$. Therefore the result of the matrix multiplication is of shape $1 \times D_x$, as desired.
    
    \item[(c)] (see code)
    \item[(d)]
\end{itemize}

\section{RNN question}
\subsection{Gradient of Softmax + CE}
\subparagraph{•}
Denote SCE - Softmax Cross Entropy function:\\
$SCE(\theta, y) = CE(\hat{y}, y) = CE(softmax(\theta), y) \\$
$= -\sum_{i} y_i log(\hat{y}_i)\\$
$if \ k \ - \ true \ label:\\$
$= - log(\hat{y}_i) = -log(softmax(\theta)_k) = -log(\frac{exp(\theta_k)}{\sum_{j}exp(\theta_j)})\\$
$= log(\sum_{j}exp(\theta_j))-\theta_k\\$
$\Longrightarrow SCE(\theta, y) = log(\sum_{j}exp(\theta_j))-\theta_k\\$

\subparagraph{•}
Now let's calculate it's derivative with respect to some $\theta_i:\\$
$\frac{\delta SCE}{\delta \theta_i} = \frac{log(\sum_{j}exp(\theta_j))}{\delta \theta_i} -\frac{\delta \theta_k}{\delta \theta_i}\\$
$\downarrow \\$
$denote: \ f \ = \sum_{j}exp(\theta_j)\\ $
$\downarrow \\$
$\frac{log(\sum_{j}exp(\theta_j))}{\delta \theta_i} -\frac{\delta \theta_k}{\delta \theta_i}\\$
$= \frac{log(f)}{\delta f}\frac{\delta f}{\delta \theta_i} -\frac{\delta \theta_k}{\delta \theta_i}\\$
$= \frac{1}{f} \sum_{j} \frac{exp(\theta_j)}{exp(\theta_i)} -\frac{\delta \theta_k}{\delta \theta_i}\\$
$= \frac{1}{f} exp(\theta_i) -\frac{\delta \theta_k}{\delta \theta_i}\\$
$= \frac{exp(\theta_i)}{\sum_{j} exp(\theta_j)}-\frac{\delta \theta_k}{\delta \theta_i}\\$
$= \frac{exp(\theta_i)}{\sum_{j} exp(\theta_j)} - \mathbbm{1}(i=k)\\$
$= softmax(\theta)_i - \mathbbm{1}(i=k)\\$
$\Longrightarrow \nabla_\theta SCE = softmax(\theta) - y = \hat{y}-y\\$ 

\subparagraph{•}
Basically the gradient of this function is prediction softmax vector minus true label one-hot vector.

\subsection{Gradients of NN}
\subparagraph{•}
In this section we use:\\
$h=\sigma(x W_1 + b_1) \\$
$\theta = h W_2 + b_2 \\$
$\hat{y}= softmax(\theta) \\$
$CE(y,\hat{y}) = -\sum_{i} y_i log(\hat{y}_i)\\$
$\nabla_\theta SCE = \hat{y}-y$

\subparagraph{•}
Also notice that Jacobian matrix of a vector function sigmoid is:\\
$da(\sigma (a))=diag(\sigma' (a))= \begin{bmatrix}
\sigma' (a_1) & 0 & 0 & 0\\
0 & \sigma' (a_2) & 0 & 0\\
0 & 0 & \sigma' (a_i) & 0\\
0 & 0 & 0 & \sigma' (a_n) 
\end{bmatrix} \ ; \ \sigma' (a_i)=\sigma(a_i)(1-\sigma(a_i))$

\subparagraph{•}
Let's write a formula for each gradient using differentials:

\subparagraph{•}
$dx(SCE)=dx(SCE(hW_2+b_2)) \\$
$= \nabla_\theta SCE \ dx(hW_2+b_2) \\$
$= \nabla_\theta SCE \ dx(hW_2) \\$
$= \nabla_\theta SCE \ W_2^T \ dx(h) \\$
$= \nabla_\theta SCE \ W_2^T \ dx(\sigma (xW_1+b_1)) \\$
$= \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ dx(xW_1+b_1) \\$
$= \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ W_1^T \ dx \\$
$\Longrightarrow \nabla_x SCE = \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ W_1^T \\$
$\Longrightarrow \nabla_x SCE = (\hat{y}-y) \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ W_1^T$

This formula won't work for batched data, but we can write it in the more generic way using Hadamard product, i.e. elementwise multiplication - $\circ$ :

$\Longrightarrow \nabla_x SCE = \{ (\hat{y}-y) \ W_2^T \ \circ\ \sigma' (xW_1+b_1)\} \ W_1^T$

\subparagraph{•}
Let's sanity check it's dimentions:\\
$x - 2 \times 10 \ $ (batch of two vectors)\\
$W_1 - 10 \times 7 \\$
$b_1 - 1 \times 7 \\$
$\sigma(xW_1+b_1) - 2 \times 7 $ (bias is broadcasted)\\
$\sigma'(xW_1+b_1) - 2 \times 7 $ (bias is broadcasted)\\
$W_2 - 7 \times 5 \\$
$b_2 - 1 \times 5 \\$
$y - 2 \times 5 $ (batch of two vectors)\\

$\Longrightarrow \nabla_x SCE = \{ (\hat{y}-y) \ W_2^T \ \circ\ \sigma' (xW_1+b_1)\} \ W_1^T\\$
$\Longrightarrow \ 2 \ \times \ 10 \  = \{ (2 \times 5) \ (5 \times 7) \ \circ\ (2 \times 7)\} \ (7 \times 10)$

All dimensions match.

\subparagraph{•}
$dW_1(SCE)=dW_1(SCE(hW_2+b_2)) \\$
$= \nabla_\theta SCE \ dW_1(hW_2+b_2) \\$
$= \nabla_\theta SCE \ dW_1(hW_2) \\$
$= \nabla_\theta SCE \ W_2^T \ dW_1(h) \\$
$= \nabla_\theta SCE \ W_2^T \ dW_1(\sigma (xW_1+b_1)) \\$
$= \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ dW_1(xW_1+b_1) \\$
$= x^T \ \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ dW_1 \\$
$\Longrightarrow \nabla_{W_1} SCE = x^T \ \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \\$
$\Longrightarrow \nabla_{W_1} SCE = x^T \ (\hat{y}-y) \ W_2^T \ diag(\sigma' (xW_1+b_1))$

Same trick with Hadamard product, i.e. elementwise multiplication - $\circ$ :\\
$\Longrightarrow \nabla_{W_1} SCE = x^T \ \{ (\hat{y}-y) \ W_2^T \ \circ \ \sigma' (xW_1+b_1) \}$

\subparagraph{•}
$db_1(SCE)=db_1(SCE(hW_2+b_2)) \\$
$= \nabla_\theta SCE \ db_1(hW_2+b_2) \\$
$= \nabla_\theta SCE \ db_1(hW_2) \\$
$= \nabla_\theta SCE \ W_2^T \ db_1(h) \\$
$= \nabla_\theta SCE \ W_2^T \ db_1(\sigma (xW_1+b_1)) \\$
$= \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ db_1(xW_1+b_1) \\$
$= \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \ 1 \ db_1 \\$
$\Longrightarrow \nabla_{b_1} SCE = \nabla_\theta SCE \ W_2^T \ diag(\sigma' (xW_1+b_1)) \\$
$\Longrightarrow \nabla_{b_1} SCE = (\hat{y}-y) \ W_2^T \ diag(\sigma' (xW_1+b_1))$

Same trick with Hadamard product, i.e. elementwise multiplication - $\circ$ :\\
$\Longrightarrow \nabla_{b_1} SCE \approx \{ (\hat{y}-y) \ W_2^T \} \ \circ \ \sigma' (xW_1+b_1)$

Notice that dims don't match, because bias was broadcasted. To match the dims, we sum the gradient along batch dimension.

$\Longrightarrow \nabla_{b_1} SCE = \sum_{batch=0}^N \{ \{ (\hat{y}-y) \ W_2^T \} \ \circ \ \sigma' (xW_1+b_1)\} _{batch}$

\subparagraph{•}
$dW_2(SCE)=dW_2(SCE(hW_2+b_2)) \\$
$= \nabla_\theta SCE \ dW_2(hW_2+b_2) \\$
$= \nabla_\theta SCE \ dW_2(hW_2) \\$
$= \nabla_\theta SCE \ h \ dW_2 \\$
$= \sigma (xW_1+b_1)^T \ \nabla_\theta SCE \ dW_2 \\$
$\Longrightarrow \nabla_{W_2} SCE = \sigma (xW_1+b_1)^T \ \nabla_\theta SCE \\$
$\Longrightarrow \nabla_{W_2} SCE = \sigma (xW_1+b_1)^T \ (\hat{y}-y)$

\subparagraph{•}
$db_2(SCE)=db_2(SCE(hW_2+b_2)) \\$
$= \nabla_\theta SCE \ db_2(hW_2+b_2) \\$
$= \nabla_\theta SCE \ 1 \ dW_2 \\$
$\Longrightarrow \nabla_{b_2} SCE = \nabla_\theta SCE \\$
$\Longrightarrow \nabla_{b_2} SCE \approx (\hat{y}-y) $

Adjust dims:\\
$\Longrightarrow \nabla_{b_2} SCE = \sum_{batch=0}^N (\hat{y}-y)_{batch} $

\section{GRU question}
\paragraph{Advantage}
Smaller discrete space - There are about 100 English-language characters in common usage if we include all punctuation marks. By contrast, a vocabulary is many thousands of words. For char-based model we need about 100 embeddings to represent all possible tokens.

\paragraph{Disadvantage}
Char-level models can generate unusual words. Word-level models can't generate mistyped words as these are not in their vocabulary. 

\section{Perplexity}

Write $p_i = p(s_i|s_1,\ldots,s_{i-1})$. Then for any $b > 0$,

$$b^{-\frac{1}{M}\sum_{i=1}^M \log_b p_i} = (b^{\sum_{i=1}^M \log_b p_i})^{-1/M} = (\prod_{i=1}^M b^{\log_b p_i})^{-1/M} = (\prod_{i=1}^M p_i)^{-1/M}$$

Therefore this expression has the same value for any such $b$, and in particular for $b=2$ and $b=e$ which are the two given expressions.


\end{document}